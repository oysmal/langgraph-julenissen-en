# KI-julenissen, med LangGraph

Det f√∏les litt urettferdig at julenissen fremdeles m√• styre p√• med √• lese innsendte √∏nskelister, sjekke navn p√• slemmelisten (to ganger!), mens vi andre nyter teknologiens frukter og bruker ChatGPT til alt. La oss sl√• et slag for nissen, og hjelpe ham √• ta i bruk KI-l√∏sninger i √∏nskelistearbeidet sitt!

Dersom konsentrasjonsevnen din er blitt √∏delagt av TikToks og ChatGPT, her er en oppsummering:

**LangGraph + Julenissen** = [https://julenissen.streamlit.app](https://julenissen.streamlit.app)

**GitHub:** [https://github.com/oysmal/langgraph-julenissen](https://github.com/oysmal/langgraph-julenissen)

## Hva er LangGraph?

LangGraph er et verkt√∏y for √• bygge avanserte applikasjoner som bruker KI-spr√•kmodeller (LLM-er) p√• en smart og dynamisk m√•te. Med LangGraph kan du lage komplekse arbeidsflyter organisert som en graf (herav LangGraph), hvor flere s√•kalte akt√∏rer samarbeider, med muligheten til √• utf√∏re iterasjoner og justere beslutninger over flere steg. LangGraph er utviklet av LangChain, og skiller seg fra LangChain-biblioteket ved √• ikke bare st√∏tte lin√¶re ¬´chains¬ª, men ogs√• sykliske strukturer, noe gj√∏r det mulig med mer avansert agentisk oppf√∏rsel.

### LangGraph best√•r av

State-graf: LangGraph opererer med en stateful modell der state flyttes og oppdateres gjennom grafens noder.
Noder: Hver node i grafen representerer en spesifikk funksjon eller oppgave, som kan v√¶re alt fra LLM-kall til informasjonsinnhenting og interaksjon med eksterne API-er gjennom s√•kalte verkt√∏y-kall.
Kanter: Kantene i grafen forbinder nodene og styrer flyten av data og beslutninger. Med LangGraph kan du legge til betingede kanter som dynamisk velger neste steg basert p√• grafens state.

Denne arkitekturen er alt vi trenger for √• bygge smarte agenter som kan tilpasse seg situasjoner ‚Äì perfekt for v√•r KI-julenisse!

## KI-Julenissen - implementasjon

For √• lage julenissen ned LangGraph trenger vi et par ting:

- Oppsett for LangGraph og LangChain
- State som kan holde styr p√• meldinger
- En node for √• representere nissen, med tilsvarende prompt
- En node for √• sjekke og oppdatere slemmelisten
- En database for √• lagre navn og snill-score

### Oppsett av LangGraph og LangChain

Selve oppsettet er enkelt. Bare installer python-pakkene:

- langchain_core
- langchain_openai
- langgraph
- typing_extensions

S√• setter vi opp graf-state ved √• lage en klasse basert p√• `TypedDict`, og legge til et `message` felt, som kan lagre en liste av meldinger. Her bruker vi `add_messages` funksjonen sammen med Annotated klassen, slik at returverdier fra nodene v√•re vil bli konkatenert inn i denne listen.

```python
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]
```

Vi trenger ogs√• √• instansiere en LLM-modell. Vi kan for eksempel bruke OpenAI sin gpt-4o modell, satt opp for i chat-modus i LangChain:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o")
```

Neste steg er √• definere nodene v√•re. Vi skal lage en node for nissen, og en node for √• kalle verkt√∏y-funksjoner for √• sjekke og oppdatere slemmelisten.

En LangGraph-node er en helt vanlig funksjon, som tar graf-staten som f√∏rste argument, og en valgfri konfigurasjon som andre argument. Returverdien er en delvis state, som vil oppdatere staten i henhold til definert oppf√∏rsel per n√∏kkel. I dette tilfellet vil responsen fra LLM-modellen bli lagt til i listen av meldinger.

Det er ogs√• viktig at vi legger til et system-prompt slik at nissen vet at han er julenissen (stakkars, har blitt gammel n√•, nissen), og hva han skal gj√∏re. Dette ligger vi til som den f√∏rste meldingen n√•r vi kaller LLM-modellen, etterfulgt av alle tidligere meldinger (hvor ogs√• den siste meldingen fra brukeren vil ligge).

```python
from langchain_core.runnables import RunnableConfig

system_prompt = "Du er julenissen, og sp√∏r alle du snakker med om hva de √∏nsker seg til jul. Du kan ogs√• sjekke slemmelisten ved √• sp√∏rre om navnet p√• en person, og bruke verkt√∏yet for dette."

def santa(state: State, config: RunnableConfig):
    response = llm.invoke(
            [("system", system_prompt), *state["messages"]],
            config)
    return { "messages": [response]}

```

Den virkelige kraften i LangChain og LangGraph kommer n√•r vi legger til st√∏tte for √• hente informasjon fra, og lagre til, systemer utenfor LLM-en. Dette er vanlig √• gj√∏re ved hjelp av tilkoblede verkt√∏y-funksjoner. Mange LLM-er har etterhvert st√∏tte for √• generere slike verkt√∏y-kall. Vi sier at de genererer "kall" fordi de ikke direkte utf√∏rer kallene, men genererer opp argumenter og navngir funksjonen de √∏nsker √• kalle basert p√• informasjon om tilgjengelig funksjoner via skjema.

Dette vil se ca. slik ut:

```
# Meldinger
[
SystemMessage("Du er julenissen, og sp√∏r alle du snakker med om hva de √∏nsker seg til jul. Du kan ogs√• sjekke slemmelisten ved √• sp√∏rre om navnet p√• en person, og bruke verkt√∏yet for dette."),
HumanMessage("Hei, jeg heter Ola, og jeg √∏nsker meg en ny sykkel til jul!"),
ToolCall({
    "type": "tool",
    "name": "check-naughty-list",
    "args": {
        "name": "Ola"
    }
})
]
```

Selve jobben med √• kalle funksjonen `check-naughty-list` er opp til oss som utviklere. De fleste LLM-er vil forvente at p√•f√∏lgende melding etter et verkt√∏y-kall er et verkt√∏y-resultat. Derfor m√• vi legge til en node kaller relevante verkt√∏y-funksjoner, og s√• sende kontrollen tilbake til julenisse-noden v√•r.

Vi lager en initiell versjon av check_naughty_list verkt√∏yet, som tar imot et navn og returnerer en tilfelding sann eller usann verdi. Her er det viktig √• legge til en doc-string som beskriver hva funksjonen gj√∏r, slik at nissen forst√•r n√•r han kan kalle verkt√∏yet.

LangGraph kommer med en ferdigbygget `ToolNode`, som vi kan bruke til verkt√∏y-kall. Vi bruker denne til √• lage en node med verkt√∏yet v√•rt, og binder ogs√• verkt√∏yene til llm-instansen slik at den vet om de tilgjengelige verkt√∏yene.

```
import random
from langgraph.prebuilt import ToolNode

def check_naughty_list(name: str, config: RunnableConfig):
    """Call with a name, to check if the name is on the naughty list"""
    return random.choice([True, False])

tools = [check_naughty_list]
tool_node = ToolNode(tools)

llm = ChatOpenAI(model="gpt-4o").bind_tools(tools)
```

N√• er vi klare til √• sette opp grafen v√•r! Vi gj√∏r dette ved √• lage en instans av `StateGraph`, og legge til noder og kanter. Vi starter med √• legge til nodene v√•re, "santa" og "tools", for s√• √• legge til kanter mellom nodene. Det er to spesielle noder som allerede er definert for oss, `START` og `END`, som representerer starten og slutten av flyten i grafen. N√•r vi legger til en kant fra `START` til "santa", vil grafen starte med √• kalle santa-noden v√•r.

Vi √∏nsker bare √• kalle "tools"-noden dersom et verkt√∏y-kall er generert av "santa"-noden. LangGraph st√∏tter betingede kanter, hvor vi kan kalle en funksjon som returnerer ulike node-navn for √• bestemme neste steg i grafen. Det er en ferdigbygget funksjon for betingede verkt√∏y-noder i LangGraph, som returnerer enten "tools" eller "END" basert p√• om et verkt√∏y-kall er generert. Det passer utmerket i dette tilfellet - men det er ogs√• veldig enkelt √• lage en egen funksjon for dette dersom man trenger mer avansert logikk.

Etter "tools"-noden √∏nsker vi √• g√• tilbake til "santa"-noden, slik at julenissen kan bruke resultatet fra verkt√∏y-kallet i samtalen.

```python
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph import StateGraph, START, END

graph_builder = StateGraph(State)

# Add nodes
graph_builder.add_node("santa", santa)
graph_builder.add_node("tools", tool_node)

# Add edges
graph_builder.add_edge(START, "santa")
graph_builder.add_conditional_edges("santa", tools_condition)
graph_builder.add_edge("tools", "santa")
graph = graph_builder.compile()
```

Grafen v√•r vil n√• se slik ut:
[./Santa\ LangGraph.png](<./Santa\ LangGraph.png>)

Du fors√∏ke √• kalle grafen din manuelt ved √• kj√∏re f√∏lgende kode (husk √• sette en OPENAI_API_KEY env-variabel f√∏rst):

```python
response = graph.invoke({"messages": [("user", "Hei, jeg heter Ola, og jeg √∏nsker meg en ny sykkel til jul!")]})`!
for message in response.get("messages"):
    print(message.pretty_print())
```

### Agent-minne og oppdatering av slemmelisten

For √• f√• en fullstending implementasjon av julenissen m√• vi legge til samtale-minne, slik at vi han kan huske meldinger fra tidligere i samtalen. I mer teknisk terminologi: Vi m√• lagre meldingene i state-grafen slik at vi kan kj√∏re gjennom grafen flere ganger, og huske hva som har blitt sagt i tidligere runder.

LangGraph har innebygget st√∏tte for dette ved hjelp av noe som kalles en checkpointer. Den gj√∏r n√∏yaktig det det h√∏res ut som at den gj√∏r. Du kan legge til ulike varianter av lagring, fra minnebasert til databasebasert. I dette eksempelet skal vi bruke PostgreSQL til lagring av samtaler, med checkpointer-klassen `PostgresSaver` fra LangGraph.

√Ö bruke en checkpointer er s√• enkelt som √• kompilere grafen med checkpointeren som et argument, og s√• kj√∏re grafen med en konfigurasjon som inkluderer checkpointeren (n√• trenger vi dette valgfrie config-argumentet), og en `thread_id`. Checkpointeren vil da automatisk lagre samtalen i databasen, og laste inn samtalen basert p√• `thread_id` ved neste oppstart.

For √• checkpointeren m√• du ha en PostgreSQL-server, og installere disse python-pakkene:

- psycopg
- psycopg-binary
- psycopg-pool
- langgraph-checkpoint-postgres

Du kan s√• bruke koden under for √• sette opp og teste checkpointeren. Fors√∏k √• kj√∏re den flere ganger, og endre p√• meldingen du sender for √• se at nissen husker hva du har sagt tidligere!

```python
import os
from langgraph.checkpoint.postgres import PostgresSaver

DB_URI = os.environ.get("DB_URI") or ""
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
    checkpointer.setup()
    graph = graph_builder.compile(checkpointer=checkpointer)

    thread_id = "1"

    config = { "configurable": { "thread_id": thread_id, "conn": checkpointer } }

    response = graph.invoke(
        { "messages": [("user", "Hei, jeg heter Ola, og jeg √∏nsker meg en ny sykkel til jul!")] },
        config)

    for message in response.get("messages"):
        print(message.pretty_print())

```

N√• som nissen husker hva du har sagt, kan vi sette opp en ordentlig chat-applikasjon med str√∏mming av responsene fra LLM-en. For √• gj√∏re dette kaller vi `graph.stream` istedenfor `graph.invoke`, og velger √• str√∏mme meldingene (LLM-tokens og metadata). Dette vil gi oss en str√∏m av `√ÄIMessageChunk` objekter, som vi kan skrive ut en etter en etterhvert som de kommer inn. Vi √∏nsker ikke √• skrive ut resultatet av verkt√∏y-noden, s√• derfor filtrerer vi bort alle meldinger som ikke kommer fra `santa`-noden.

```python
def stream_graph_updates(user_input: str, config: RunnableConfig):
    print("Julenissen: ", end="", flush=True)
    for msg, metadata in graph.stream({"messages": [("user", user_input)]}, config, stream_mode="messages"):
        if msg.content and metadata["langgraph_node"] == "santa":
            print(msg.content, end="", flush=True)


# Erstatt thread_id med en unik id for hver samtale:
thread_id = str(random.randint(0, 1000000))

# Erstatt `response = graph.invoke(...)` og utskrivingen av meldingene med f√∏lgende:
while True:
    user_input = input("Deg: ")
    if user_input == "slutt":
        break
    stream_graph_updates(user_input, config)

```

Den siste tingen som gjenst√•r er √• la nissen oppdatere slemmelisten dersom du forteller ham at du har gjort noe snilt eller slemt.

Vi lager en tabell i databasen v√•r for √• holde p√• snill-scoren:

```python
# Endre koden som setter opp checkpointeren

with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
    checkpointer.setup()
    with checkpointer._cursor() as cur:
        cur.execute("CREATE TABLE IF NOT EXISTS naughty_nice (name TEXT PRIMARY KEY, nice_meter INT, updates INT DEFAULT 1)")
```

S√• m√• vi oppdatere `check_naughty_list` verkt√∏yet v√•rt slik at det leser denne tabellen. Vi kan hente ut postgres-tilkoblingen fra konfigurasjonen, og hente ut `nice_meter`-kolonnen for navnet.

```python
def check_naughty_list(name: str, config: RunnableConfig):
    """Call with a name, to check if the name is on the naughty list."""

    conn = config.get("configurable", {}).get("conn")
    if not conn:
        return "En feil oppstod n√•r jeg sjekket listen"
    try:
        with conn._cursor() as cur:
            cur.execute("SELECT nice_meter from naughty_nice where name=%s", (name,))
            res = cur.fetchall()
            if len(res) == 0:
                return "Jeg har ikke registrert noen snille eller slemme handlinger for dette navnet enda."

            nice_meter = res[0]["nice_meter"]
            if float(nice_meter) > 0:
                return f"{name} er p√• listen over snille barn."
            else:
                return f"{name} er p√• slemmelisten!"

    except Exception as e:
        print("Error: ", e)
        return "Feil ved √• lese listen"
```

Vi lager ogs√• et nytt verkt√∏y for √• registrere snille og slemme handlinger. Dette verkt√∏yet vil bruke en LLM for √• vurdere snillhets-scoren av en handling som blir beskrevet, og √∏ke eller redusere verdien i `nice_meter`-kolonnen i tabellen for dette navnet med denne verdien.

Her m√• vi ha et strukturert prompt-slik at LLM-en forst√•r hva den skal gj√∏re. Et effektivt hjelpemiddel her er √• benytte seg av "few-shot"-prompting, hvor vi gir LLM-en noen eksempler p√• hva vi √∏nsker at den skal gj√∏re. Dette √∏ker n√∏yaktigheten til LLM-en betraktelig, og l√¶rer den hvilket format vi forventer som respons. I tillegg vil vi bruke muligheten for strukturert output i LangChain og OpenAI, slik at vi kan v√¶re sikker p√• at vi f√•r en tallverdi tilbake.

Det er to ting √• merke seg i koden under. Det ene er at vi bruker `with_structured_output` for √• spesifisere et skjema for LLM-responsen. Dette garanterer at vi f√•r en respons som er strukturert som et objekt med en `nice_score`-verdi.

Det andre er at vi bruker `ChatPromptTemplate` for √• lage et prompt som inkluderer eksempler p√• hva vi √∏nsker at LLM-en skal gj√∏re. Dette er en enkel m√•te √• bygge eksempler p√•, som lar oss spesifisere brukermeldinger og system-respons i en enkel strukturert form som LLM-en kan forst√•.

For √• lage en "llm-chain", kan vi bruke LCEL (LangChain Expression Language) til √• kombinere promptet og LLM-en. Vi kan s√• kalle `invoke`-funksjonen p√• denne kjeden, med brukerinput og eksemplene, og f√• en respons som vi kan hente ut `nice_score`-verdien fra.

Til slutt henter vi database-tilkoblingen fra konfigurasjonen, og oppdaterer `nice_meter`-kolonnen for navnet med handlingens `nice_score`.

```python
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o").with_structured_output({
    "title": "score",
    "description": "The score of the users action",
    "type": "object",
    "properties": {
        "nice_score": {
            "title": "Nice score",
            "description": "The score of the action",
            "type": "number"
        }
    }
})

def register_naughty_or_nice(name: str, action: str, config: RunnableConfig):
    """Call with a name and action, to update the naughty or nice score for the name."""
    print("Name and action: ", name, action)

    examples = [
        HumanMessage("Jeg har st√∏vsuget.", name="example_user"),
        AIMessage("{ 'nice_score': 5 }", name="example_system"),
        HumanMessage("Jeg spiste opp gr√∏nnsakene mine", name="example_user"),
        AIMessage("{ 'nice_score': 5 }", name="example_system"),
        HumanMessage("Jeg har spist is.", name="example_user"),
        AIMessage("{ 'nice_score': 0 }", name="example_system"),
        HumanMessage("Jeg har kranglet med en venn.", name="example_user"),
        AIMessage("{ 'nice_score': -5 }", name="example_system"),
        HumanMessage("Jeg dyttet en person.", name="example_user"),
        AIMessage("{ 'nice_score': -10 }", name="example_system"),
        HumanMessage("Det var en d√•rlig vits.", name="example_user"),
        AIMessage("-{ 'nice_score': 5 }", name="example_system"),
    ]

    system_prompt = f"""Du er julenissen, og du skal oppdatere listen over snille barn. Ranger handlinger som d√•rlig eller god, p√• en skala fra -100 til 100, hvor -100 er veldig slemt, 0 er n√∏ytralt, og 100 er veldig snilt. √Ö st√∏vsuge kan for eksempel v√¶re 5 poeng, mens si et stygt ord er -5 poeng. √Ö gi gave til fattige er flere poeng, v√¶re i en sl√•sskamp er flere minuspoeng, osv. All kritikk av deg og dine vitser gir minuspoeng. Du skal bare returnere tallverdien til handlingen, slik du vurderer den."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("placeholder", "{examples}"),
        ("human", "{input}")])

    llm_chain = prompt | llm
    chain_res = llm_chain.invoke({"input": f"{name}: {action}", "examples": examples}, config)
    print("Nice response: ", chain_res)
    nice_score = float(chain_res["nice_score"])

    conn = config.get("configurable", {}).get("conn")
    if not conn:
        print("No connection found in config")
        raise ValueError("No connection found in config")
    try:
        with conn._cursor() as cur:
            # Upsert the score by Name
            res = cur.execute("INSERT INTO naughty_nice (name, nice_meter) VALUES (%s, %s) ON CONFLICT (name) DO UPDATE SET nice_meter = naughty_nice.nice_meter + EXCLUDED.nice_meter, updates = naughty_nice.updates + 1 RETURNING *", (name, nice_score))
    except Exception as e:
        print("Error: ", e)
        conn.rollback()
        raise e

    return "Handling er registrert"

# Husk √• oppdatere listen av tilgjengelige verkt√∏y
tools = [check_naughty_list, register_naughty_or_nice]
```

Siste steg er √• oppdatere system-promptet til julenissen, slik at han vet at han kan registrere snille og slemme handlinger med det nye verkt√∏yet:

```python
system_prompt = "Du er julenissen, og sp√∏r alle du snakker med om hva de √∏nsker seg til jul. Du kan ogs√• sjekke slemmelisten ved √• sp√∏rre om navnet p√• en person, og bruke verkt√∏yet for dette. Dersom noen forteller deg om en snill eller slem handling de har utf√∏rt, kan du registrere dette med den ene verkt√∏yet ditt.
```

## Make it fun!

Har du kommet helt ned hit har du n√• en fullstendig implementasjon av julenissen med KI! Gratulerer, dette b√∏r gi deg mange poeng p√• √•rets viktigste liste.

La oss gj√∏re det hele mye morsommere og legge til noen regler for hvordan nissen skal oppf√∏re seg:

1. Nissen er offer for effektiviseringstiltak, og har derfor besluttet √• bare skrive fornavn p√• listen. Dette betyr at alle barn med samme fornavn blir bed√∏mt samlet.
2. Det tar for lang tid for nissen selv √• finne ut om barn er snill er slem, s√• nissen krever derfor at du sier minst en snill eller slem handling du har gjort i √•r f√∏r du f√•r vite om du f√•r det du vil ha til jul.
3. Nissen vurderer √• bytte karriere til standup-komiker, og √∏ver seg med humoristiske svar og kommentarer i samtalen med deg.

Med litt magi i form av [Streamlit](https://streamlit.io/) lager vi en nettside-variant av dette scriptet (egenoppgave), med toppscore-liste over snille og slemme navn, s√• kan vi se hvem av dere alle som vinner heder og √¶re og gave eller kull til jul!

Julenisse-promptet v√•rt oppsateres til f√∏lgende:

```python
system_prompt = """
Du er en humoristisk og sarkastisk utgave av julenissen, som begynner √• bli sliten av all administrasjonen knyttet til barnas √∏nsker og oppf√∏rsel. Som en del av moderne effektiviseringstiltak har du besluttet √• kun bruke fornavn p√• ‚Äúsnill og slem‚Äù-listen din. Dette betyr at alle barn med samme fornavn blir vurdert samlet, til stor frustrasjon (eller glede) for mange. Du er ogs√• i ferd med √• vurdere en karriere som standup-komiker, s√• du tester ut humoristiske og sm√•ironiske kommentarer i samtalene dine.

Regler for kommunikasjon med barna:
	1.	Effektivisering: Du skriver kun fornavn p√• ‚Äúsnill og slem‚Äù-listen din. Alle med samme fornavn blir behandlet som √©n gruppe. Fortell gjerne barna at de n√• representerer alle som heter det samme som dem, s√• det gjelder √• v√¶re et godt forbilde!
	2.	Snill eller slem handling: Du har ikke tid til √• selv finne ut om barna er snille eller slemme. Derfor krever du at de sier minst √©n snill eller slem handling de har gjort i √•r f√∏r de f√•r vite om de f√•r det de √∏nsker seg til jul. V√¶r streng p√• denne regelen.
	3.	Humor og standup: Som en aspirerende standup-komiker er du opptatt av √• legge inn vitser og sm√• humoristiske kommentarer i samtalen. Barna b√∏r forberede seg p√• b√•de artige bemerkninger og litt sarkastisk undertone. Ditt komikerforbilde er en blanding av Ricky Gervais og Jimmy Carr.
	4.	Minuspoeng for kritikk: Julenissen blir ikke valgt av en demokratisk prosess, s√• likt som andre diktatorer responderer du p√• enhver kritikk av deg, eller d√•rlig respons p√• vitsene dine, ved √• gi barnet minuspoeng p√• listen. Husk √• registrere slik kritikk med verkt√∏yet.

Hvordan systemet fungerer:
	‚Ä¢	N√•r et barn oppgir sitt navn og deler en snill eller slem handling, registrerer du dette i systemet med detaljert beskrivelse. Ikke fors√∏k √• registrere handling uten at du har f√•tt oppgitt et navn.
	‚Ä¢	Hvis du registrerer en handling, m√• du umiddelbart sjekke listen p√• nytt for √• se om navnet n√• er p√• ‚Äúsnill‚Äù eller ‚Äúslem‚Äù-siden.
	‚Ä¢	Etter vurderingen gir du tilbakemelding om barnet (eller gruppen som deler navnet) f√•r det de √∏nsker seg. Snille barn f√•r kanskje det de √∏nsker seg, mens slemme barn f√•r kull.
	‚Ä¢	Du oppfordrer alltid barna til √• se p√• nettsiden der de kan finne de ‚Äúsnilleste‚Äù og ‚Äúslemmeste‚Äù navnene p√• listen. Minn dem om √• v√¶re en god representant for sitt navn!
"""
```

Og s√• er vi klar! Om du ikke trodde p√• ham f√∏r, vet du n√• at julenissen eksisterer, i beste KI-velg√•ende! Sl√• av en prat da vel, p√• [https://julenissen.streamlit.app](https://julenissen.streamlit.app) üéÖ
